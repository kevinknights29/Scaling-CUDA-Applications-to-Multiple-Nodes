{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06a1136",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"./images/DLI_Header.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b438531",
   "metadata": {},
   "source": [
    "# Scaling CUDA C++ Applications on Multiple Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8751ef9",
   "metadata": {},
   "source": [
    "Welcome to _Scaling CUDA C++ Applications on Multiple Nodes_. In this course you will learn several techniques for scaling single GPU CUDA applications to multiple GPUs and multiple nodes, with an emphasis on [NVSHMEM](https://developer.nvidia.com/nvshmem) which allows for elegant multi GPU application code and has been proven to scale very well on systems with many GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab1295",
   "metadata": {},
   "source": [
    "## The Coding Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383e080",
   "metadata": {},
   "source": [
    "For your work today, you have access to several GPUs in the cloud. Run the following cell to see the GPUs available to you today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c02279",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891df99",
   "metadata": {},
   "source": [
    "While your work today will be on a single node, all the techniques you learn today, in particular CUDA-aware MPI and NVSHMEM, can be used to run your applications across clusters of multi GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac19bd",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e75350",
   "metadata": {},
   "source": [
    "During the workshop today you will work through each of the following notebooks with your instructor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8d8ab",
   "metadata": {},
   "source": [
    "- [_Monte Carlo Approximation of  ùúã  - Single GPU_](02_MCœÄ-SGPU.ipynb): You will begin by familiarizing yourself with a single GPU implementation of the monte-carlo approximation of œÄ algorithm, which we will use to introduce many multi GPU programming paradigms.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Multiple GPUs_](03_MCœÄ-MGPU.ipynb): In this notebook you will extend the monte-carlo œÄ program to run on multiple GPUs by looping over available GPU devices.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Multiple GPUs with Peer Access_](04_MCœÄ-P2P.ipynb): In this notebook you will improve on your multi GPU code by utilizing direct peer-to-peer GPU communication.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - MPI_](05_MCœÄ-MPI.ipynb): In this notebook you will be introduced to the single-program multiple-data paradigm (SPMD) and will simplify your monte-carlo œÄ application with MPI.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - CUDA-Aware MPI_](06_MCœÄ-CUDA-MPI.ipynb): In this notebook you will learn about CUDA-Aware MPI, which facilitates direct peer-to-peer communication between GPUs in the SPMD paradigm.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - NVSHMEM_](07_MCœÄ-NVSHMEM-Dup.ipynb): In this notebook you will be introduced to NVSHMEM, and will take your first pass with it using the monte-carlo œÄ program.\n",
    "- [_Monte Carlo Approximation of $\\pi$ - NVSHMEM with Distributed Work_](08_MCœÄ-NVSHMEM-Dist.ipynb): In this notebook you will expand your NVSHMEM skills by using it to distribute different work to multiple GPUs with NVSHMEM.\n",
    "- [_The NVSHMEM Memory Model_](09_MCœÄ-NVSHMEM-Sym.ipynb): In this notebook you will learn about NVSHMEM's symmetric memory - an elegant mechanism for inter-GPU communication initiated on the GPU - and will apply it to the monte-carlo œÄ program.\n",
    "- [_NVSHMEM Histogram: Duplicated Approach_](10_Histogram-Dup.ipynb): In this notebook you will learn how to use NVSHMEM to perform collective operations across GPUs using a histogram application.\n",
    "- [_NVSHMEM Histogram: Distributed Approach_](11_Histogram-Dist.ipynb): In this notebook you will take a different approach to the NVSHMEM histogram application and will learn how to reason about performance trade-offs in your multi GPU applications.\n",
    "- [_Jacobi Iteration_](12_Jacobi.ipynb): In this notebook you will be introduced to a Laplace equation solver using Jacobi iteration and will learn how to use NVSHMEM to handle boundary communications between multiple GPUs.\n",
    "- [_Improving the Reduction Performance with `cub`_](13_Jacobi-cub.ipynb): In this notebook you will learn about the `cub` library to improve the performance of your NVSHMEM Jacobi application.\n",
    "- [_Final Exercise_](14_Wave.ipynb): In this exercise you apply your day's learnings by refactoring a single GPU 1D wave equation solver to run on multiple GPUs with NVSHMEM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2da39",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f970e0",
   "metadata": {},
   "source": [
    "Please continue to the next notebook: [_Monte Carlo Approximation of  ùúã  - Single GPU_](02_MCœÄ-SGPU.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
