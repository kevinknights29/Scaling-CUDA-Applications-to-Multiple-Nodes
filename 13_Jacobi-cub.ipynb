{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"./images/DLI_Header.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Reduction Performance with `cub`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduce the `cub` library and utilize it in the Jacobi iteration code to greatly improve the performance of the `jacobi` kernel. This notebook is a bit of an aside from the main focus of this course, however, the gains made from using highly optimized libraries like `cub` make it worth your being exposed to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be able to use the `cub` library to perform optimized block level reductions, greatly reducing atomic pressure on symmetric data being used by all GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Atomic Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, hundreds of thousands of threads made atomic writes to `l2_norm`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "int main()\n",
    "{\n",
    "    ...\n",
    "\n",
    "    float* d_l2_norm = (float*) nvshmem_malloc(sizeof(float));\n",
    "    jacobi<<<blocks, threads_per_block>>>(f_old, f, d_l2_norm, N); // Grid launched with hundreds of thousands of threads.\n",
    "}\n",
    "\n",
    "__global__ void jacobi (const float* f_old, float* f, float* l2_norm, int N)\n",
    "{\n",
    "    ...\n",
    "\n",
    "    float l2 = (f[idx] - f_old[idx]) * (f[idx] - f_old[idx]);\n",
    "    // Here in the kernel, (practically) every thread is making atomic writes to symmetric `l2_norm`.\n",
    "    atomicAdd(l2_norm, l2);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we have hundreds of thousands of threads performing an atomic reduction to the same variable. Knowing this we might propose that there is a great deal of atomic serialization in our program, which could have a significant impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile with Nsight Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nsight Compute](https://developer.nvidia.com/nsight-compute) is a kernel profiling tool, and here we will use its command line tool `ncu` to profile the solution code from the previous notebook.\n",
    "\n",
    "`profile_one_jacobi_PE.sh` is a simple script that profiles only PE 0 (and skips the first few kernels to allow the GPU to warm up). Run the following cell to compile and run the solution application, generating profiling output for the first PE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!nvcc -x cu -arch=sm_70 -rdc=true -I $NVSHMEM_HOME/include -L $NVSHMEM_HOME/lib -lnvshmem -lcuda -o jacobi_solution_step1 solutions/jacobi_step1.cpp\n",
    "!nvshmrun -np $NUM_DEVICES ./code/profile_one_jacobi_PE.sh ./jacobi_solution_step1\n",
    "!ncu -i report.ncu-rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is that despite reasonable achieved occupancy (`Section:Occupancy` -> `Achieved Occupancy`), we are nowhere near peak theoretical memory bandwidth (`Section: GPU Speed Of Light` -> `Memory [%]`).\n",
    "\n",
    "This supports our earlier hypothesis, and is likely because we have hundreds of thousands of threads performing an atomic reduction to the same variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Reduce the Atomic Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach to limit the amount of atomic serialization is to [perform as much of the reduction within the block as possible](https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/). In such an approach, each block would efficiently reduce each of its threads' `l2` values, and then, have only one thread for each block, perform the atomic add to the symmetrical `l2_norm` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Level Reduction with `cub`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cub](https://docs.nvidia.com/cuda/cub/index.html) is a header library provided by NVIDIA as part of CUDA[<sup>1</sup>](#footnote1) that provides interfaces for primitive operations that are often used in kernels such as reductions and scan operations.\n",
    "\n",
    "For our current situation, let's use the [BlockReduce](https://nvlabs.github.io/cub/classcub_1_1_block_reduce.html) interface in `cub` to perform block-level reductions, and then only have thread `0` in each block perform the atomic add to the symmetric data `l2_norm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `cub::BlockReduce`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `cub` we first add the header:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "#include <cub/cub.cuh>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `BlockReduce` we then define a `BlockReduce` type with as many threads per block as we want to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "typedef cub::BlockReduce<float, 256> BlockReduce;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BlockReduce` will be utilizing shared memory to perform its efficient block-level reduction, so next we allocate shared memory for it to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "__shared__ typename BlockReduce::TempStorage temp_storage;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we perform the reduction, in our case a sum reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "float reduced_value = BlockReduce(temp_storage).Sum(value_to_reduce);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Use Block Reduce in Jacobi Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, look for the FIXMEs to refactor [exercises/jacobi_step2.cpp](exercises/jacobi_step2.cpp) (which starts from the solution of the last notebook) to perform block reduction with `cub`, only performing the atomic write to the symmetric `l2_norm` with from thread `0` in each block.\n",
    "\n",
    "Check [solutions/jacobi_step2.cpp](solutions/jacobi_step2.cpp) if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -x cu -arch=sm_70 -rdc=true -I $NVSHMEM_HOME/include -L $NVSHMEM_HOME/lib -lnvshmem -lcuda -o jacobi_step2 exercises/jacobi_step2.cpp\n",
    "!nvshmrun -np $NUM_DEVICES ./jacobi_step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile with Nsight Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, if you weren't able to complete the exercise on your own, replace `exercises/jacobi_step2.cpp` with `solutions/jacobi_step2.cpp` in the compilation cell above, and run the cell so that you have a working solution to profile below.\n",
    "\n",
    "Let's verify with Nsight Compute that we achieve a significantly higher percentage of peak throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvshmrun -np $NUM_DEVICES ./code/profile_one_jacobi_PE.sh ./jacobi_step2\n",
    "!ncu -i report.ncu-rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that `Section: GPU Speed Of Light` -> `Memory [%]` has greatly improved and that `Section: GPU Speed Of Light` -> `Elapsed Cycles ` has greatly decreased. Interestingly we see in `Section: Launch Statistics` -> `Static Shared Memory Per Block` the shared memory we allocated for use by `BlockReduce`'s efficient reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobi iteration stencil is relatively simple, and we only looked at it in one dimension, but methods like this one are omnipresent in scientific computing. We can now see how straightforward it is to use NVSHMEM in problems that use a standard domain decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next and final notebook, we present you with a fully functional single GPU CUDA implementation of a 1D wave function solver, and ask you to combine all your skills thus far to refactor it using NVSHMEM.\n",
    "\n",
    "Please open the next notebook: [_Final Exercise_](14_Wave.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a much deeper dive into Jacobi Iteration using a number of multi GPU techniques, please visit [github.com/NVIDIA/multi-gpu-programming-models](https://github.com/NVIDIA/multi-gpu-programming-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes\n",
    "\n",
    "<span id=\"footnote1\">1</span>: Starting with CUDA 11.0, cub is available as a first-class citizen of the CUDA toolkit available directly in its `include/` directory. In prior versions of CUDA, cub was bundled as part of Thrust in the CUDA toolkit. In either case, it is [available on GitHub](https://github.com/NVIDIA/cub)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
