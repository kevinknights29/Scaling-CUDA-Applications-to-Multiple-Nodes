{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"./images/DLI_Header.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVSHMEM Histogram: Duplicated Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you are going to use NVSHMEM to run a histogram program. You will be copying the full histogram to every GPU, and then doing an in-place reduction for the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be able to write a fully functional histogram program that operates over multiple GPUs using NVSHMEM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a problem that is somewhat similar to the previous one but slightly more complicated: constructing a histogram. That is, given an array of $N$ integers, and a set of $M$ integer ranges, count how many elements in the array belong in each of the $M$ ranges. Without much loss of generality we will specify that the integers are positive in the range $[0, K-1]$ and that the ranges or buckets are evenly linearly spaced (and for simplicity $K$ is evenly divisible by $M$), so that the first bucket covers $[0, K\\, /\\, M - 1]$, the second covers $[K\\, /\\, M, 2K\\, /\\, M - 1]$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin refactoring the code for multiple GPUs, we'll start with an example of the code using a single GPU. The simplest way to solve this problem is again using atomics. We loop through the array and, for each element in the array, calculate which bucket it should fall into; given some integer $n$ the index of the histogram array it belongs to is $(nM)\\, /\\, K$. We then atomically increment the counter for that bucket. Inspect [the code](code/histogram.cpp) and then run it to see what kind of output we get. Feel free to adjust the parameters if you like (though be careful about 32-bit integer overflow if you make the numbers too large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -x cu -arch=sm_70 -o histogram code/histogram.cpp\n",
    "!./histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVSHMEM Implementation for the Duplicated Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to distribute this across multiple GPUs would be the same approach we used for the $\\pi$ estimator: given $N$ integers we can distribute them evenly across our GPUs. Then we can do a reduction across all PEs. We will call this the \"duplicated\" approach since a copy of the full histogram exists on all GPUs. We will name the first step that increments the histogram buckets the \"tabulation\" step, and the second step that combines the results across all PEs the \"combination\" step and time them separately (for comparison against the next method).\n",
    "\n",
    "<center><img src=\"images/histogram_duplicated_approach.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the reduction API [nvshmem_int_sum_reduce()](https://docs.nvidia.com/nvshmem/api/gen/api/collectives.html#sum) to reduce over all buckets of the histogram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "nvshmem_int_sum_reduce(team, destination, source, nelems);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `destination == source`, then this becomes an in-place reduction and this is legal to do in NVSHMEM; this has the benefit of being cleaner than creating a temporary destination buffer, so we recommend doing so in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: NVSHMEM Multi GPU Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the FIXMEs in [exercises/histogram_step1.cpp](exercises/histogram_step1.cpp) to guide your work.\n",
    "\n",
    "Feel free to consult [the solution](solutions/histogram_step1.cpp) if you need help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -x cu -arch=sm_70 -rdc=true -I $NVSHMEM_HOME/include -L $NVSHMEM_HOME/lib -lnvshmem -lcuda -o histogram_step1 exercises/histogram_step1.cpp\n",
    "!nvshmrun -np $NUM_DEVICES ./histogram_step1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we will continue with our histogram program. This time, instead of duplicating the histogram across GPUs and then reducing it, we will distribute parts of the histogram to each GPU and concatenate them. In addition to increasing our NVSHMEM capabilities, the refactor will also give us a chance to observe performance trade-offs we should consider when distributing work to multiple GPUs.\n",
    "\n",
    "Please open the next notebook: [_NVSHMEM Histogram: Distributed Approach_](11_Histogram-Dist.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
